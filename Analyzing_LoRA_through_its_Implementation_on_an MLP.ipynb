{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Set the stage - Data, Model, Library, Pre-training\n",
        "\n",
        "Huggingface has developed the `peft` library to facilitate the efficient adaptation of pretrained language models for various downstream applications without fine-tuning all of the model's parameters. The `peft` library supports multiple fine-tuning methods one of which is LoRA (Low Rank Adaptators) and it can be applied to various model types, not limited to transformers. Currently, `peft` allows fine-tuning of Linear, Embedding, and Conv2D layers in conjunction with LoRA.\n",
        "\n",
        "There is an abundance of tutorials and blogs discussing how to implement LoRA fine-tuning to Large Language Models (LLMs) such as LLaMa and alike. Grasping the methodology and rationale behind LoRA while applying to large models is challenging because of the inherent complexity of the models. To enhance our understanding, lets implement LoRA in a multilayer perceptron (MLP) and use it to train a model for a binary classification task and thereby also assess parameter efficiency during the fine-tuning process."
      ],
      "metadata": {
        "id": "sqAv7sMIpvOB"
      },
      "id": "sqAv7sMIpvOB"
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "qLiGTLag5iVW"
      },
      "id": "qLiGTLag5iVW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(0)"
      ],
      "metadata": {
        "id": "xLDt9WWnUJFc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7a1f292-1706-4c66-9f67-24a5053c34f9"
      },
      "id": "xLDt9WWnUJFc",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e03e06b9610>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b127a78"
      },
      "source": [
        "Let's create a toy dataset consisting of random data for a classification task. There is a little bit of signal in the data, so we should expect that the loss of the model can improve during training."
      ],
      "id": "2b127a78"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b355567e"
      },
      "outputs": [],
      "source": [
        "X = torch.rand((1000, 20)) # Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1).\n",
        "y = (torch.sin(X.sum(1)) > 0).long() # y is the label with shape (1000, 1) which results in 1 if the sin of sum of elements in each row is > 0.5 and 0 otherwise. y is then cast to (torch.int64)."
      ],
      "id": "b355567e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Distribution of data between both classes\n",
        "unique, counts = torch.unique(y, return_counts=True)\n",
        "distribution = dict(zip(unique.tolist(), counts.tolist()))\n",
        "distribution"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v54HcFP23ylo",
        "outputId": "d416c7e9-56ef-4e5e-8df8-e92e05f01c87"
      },
      "id": "v54HcFP23ylo",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 663, 1: 337}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a60a869d"
      },
      "outputs": [],
      "source": [
        "n_train = 800\n",
        "batch_size = 64"
      ],
      "id": "a60a869d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8859572e"
      },
      "outputs": [],
      "source": [
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[:n_train], y[:n_train]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[n_train:], y[n_train:]),\n",
        "    batch_size=batch_size,\n",
        ")"
      ],
      "id": "8859572e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "97bddd2c"
      },
      "source": [
        "As a model, we use a simple multilayer perceptron (MLP). For demonstration purposes, we use a very large number of hidden units. This is totally an overkill for this task but it helps to demonstrate the advantages of `peft`. In more realistic settings, models will also be quite large on average, so this is not far-fetched."
      ],
      "id": "97bddd2c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b43cd8f"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Linear(20, 2000),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(2000, 200),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(200, 2),\n",
        "            nn.LogSoftmax(dim=-1), #log loss / binary cross entropy loss\n",
        "        )\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self.seq(X)"
      ],
      "id": "1b43cd8f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1277bf00"
      },
      "source": [
        "Here are just a few training hyper-parameters and a simple function that performs the training and evaluation loop."
      ],
      "id": "1277bf00"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5d14c0c4",
        "outputId": "5bafc762-d5dd-4662-fcd1-cf1fef1117c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "lr = 0.002\n",
        "batch_size = 64\n",
        "max_epochs = 35\n",
        "device = 'cpu' if not torch.cuda.is_available() else 'cuda'\n",
        "print(device)"
      ],
      "id": "5d14c0c4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "657d6b3e"
      },
      "outputs": [],
      "source": [
        "# Data and Model have to put on the device.\n",
        "# So, xb, yb and model are on device.\n",
        "def train(model, optimizer, criterion, train_dataloader, eval_dataloader, epochs):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for xb, yb in train_dataloader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            train_loss += loss.detach().float()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        model.eval()\n",
        "        eval_loss = 0\n",
        "        for xb, yb in eval_dataloader:\n",
        "            xb = xb.to(device)\n",
        "            yb = yb.to(device)\n",
        "            with torch.no_grad():\n",
        "                outputs = model(xb)\n",
        "            loss = criterion(outputs, yb)\n",
        "            eval_loss += loss.detach().float()\n",
        "\n",
        "        eval_loss_total = (eval_loss / len(eval_dataloader)).item()\n",
        "        train_loss_total = (train_loss / len(train_dataloader)).item()\n",
        "        print(f\"{epoch=:<2}  {train_loss_total=:.4f}  {eval_loss_total=:.4f}\")"
      ],
      "id": "657d6b3e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f059ced4"
      },
      "outputs": [],
      "source": [
        "base_model = MLP().to(device)\n",
        "optimizer = torch.optim.Adam(base_model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "id": "f059ced4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJD-3tGW93IR",
        "outputId": "36fff6a1-d54b-49f2-8a37-d1d95cfa788f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (seq): Sequential(\n",
              "    (0): Linear(in_features=20, out_features=2000, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=2000, out_features=200, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=200, out_features=2, bias=True)\n",
              "    (5): LogSoftmax(dim=-1)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "base_model"
      ],
      "id": "NJD-3tGW93IR"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jIyHe3KZ745s"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "id": "jIyHe3KZ745s"
    },
    {
      "cell_type": "code",
      "source": [
        "print_trainable_parameters(base_model)"
      ],
      "metadata": {
        "id": "8538QGChubaZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dace34e7-40ce-4c95-c251-418da915aa5c"
      },
      "id": "8538QGChubaZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 442602 || all params: 442602 || trainable%: 100.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "17698863",
        "outputId": "c33e989d-8f1f-471e-882c-e0b72b794ddb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0   train_loss_total=0.6282  eval_loss_total=0.5629\n",
            "epoch=1   train_loss_total=0.5087  eval_loss_total=0.4953\n",
            "epoch=2   train_loss_total=0.3993  eval_loss_total=0.4251\n",
            "epoch=3   train_loss_total=0.3303  eval_loss_total=0.4426\n",
            "epoch=4   train_loss_total=0.3449  eval_loss_total=0.3773\n",
            "epoch=5   train_loss_total=0.3435  eval_loss_total=0.3848\n",
            "epoch=6   train_loss_total=0.2730  eval_loss_total=0.3268\n",
            "epoch=7   train_loss_total=0.2616  eval_loss_total=0.3372\n",
            "epoch=8   train_loss_total=0.2213  eval_loss_total=0.3446\n",
            "epoch=9   train_loss_total=0.2236  eval_loss_total=0.3269\n",
            "epoch=10  train_loss_total=0.2067  eval_loss_total=0.3505\n",
            "epoch=11  train_loss_total=0.1907  eval_loss_total=0.3439\n",
            "epoch=12  train_loss_total=0.1981  eval_loss_total=0.3375\n",
            "epoch=13  train_loss_total=0.1900  eval_loss_total=0.3734\n",
            "epoch=14  train_loss_total=0.1905  eval_loss_total=0.3414\n",
            "epoch=15  train_loss_total=0.1686  eval_loss_total=0.4842\n",
            "epoch=16  train_loss_total=0.1666  eval_loss_total=0.3911\n",
            "epoch=17  train_loss_total=0.1451  eval_loss_total=0.3675\n",
            "epoch=18  train_loss_total=0.1417  eval_loss_total=0.4038\n",
            "epoch=19  train_loss_total=0.1152  eval_loss_total=0.3991\n",
            "CPU times: user 5.08 s, sys: 676 ms, total: 5.75 s\n",
            "Wall time: 11.8 s\n"
          ]
        }
      ],
      "source": [
        "# Lets train the base model\n",
        "%time train(base_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=20)"
      ],
      "id": "17698863"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cef0029"
      },
      "source": [
        " We achieved an evaluation loss that is better than a random outcome. In fine-tuning exercises, the primary focus is typically on the model's performance for a specific downstream task. It's important to note that showcasing performance improvements with LoRA fine-tuning on our current MLP model pretrained on toy dataset may not be ideal, mainly because its advantages are more pronounced in larger models with clearly defined downstream tasks. Nonetheless, our objective here is to assess how LoRA enhances parameter efficiency and to deepen our understanding of its algorithm. This exploration is particularly relevant because comprehending the implementation of LoRA at code level on large models can be quite challenging."
      ],
      "id": "4cef0029"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4f106078"
      },
      "source": [
        "## Finetuning with LoRA\n",
        "Using our model, pre-trained for 20 epochs as a base_model, we'll apply LoRA using Huggingface's `peft` library. Make sure that you have the latest version of peft installed. We already established that we will be injecting few extra set of parameters called adapters in between the layers of the pre-trained based model, focusing on training only these adapters while keeping the base model's parameters frozen. Where and How to inject the adapters in the base model is still an open question."
      ],
      "id": "4f106078"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KWWE0BvghKmT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4806931-feaa-48db-a800-bb69c11b40b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting peft\n",
            "  Downloading peft-0.7.1-py3-none-any.whl (168 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.3/168.3 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from peft) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from peft) (23.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from peft) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from peft) (2.1.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from peft) (4.35.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from peft) (4.66.1)\n",
            "Collecting accelerate>=0.21.0 (from peft)\n",
            "  Downloading accelerate-0.26.1-py3-none-any.whl (270 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m270.9/270.9 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from peft) (0.4.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.17.0 in /usr/local/lib/python3.10/dist-packages (from peft) (0.20.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2023.6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->peft) (2.1.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (2023.6.3)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->peft) (0.15.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
            "Installing collected packages: accelerate, peft\n",
            "Successfully installed accelerate-0.26.1 peft-0.7.1\n"
          ]
        }
      ],
      "source": [
        "!python -m pip install --upgrade peft"
      ],
      "id": "KWWE0BvghKmT"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4d9da3d9"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import os\n",
        "\n",
        "# Set this variable to 1 to not get default messages. Ignore bnb warnings\n",
        "os.environ[\"BITSANDBYTES_NOWELCOME\"] = \"1\""
      ],
      "id": "4d9da3d9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "44075f54"
      },
      "outputs": [],
      "source": [
        "import peft"
      ],
      "id": "44075f54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We already estrablished that we will be injecting few extra set of parameters called adaptors in between the layers of the pretrained based model, focusing on training only these adaptors while keeping the base model's parameters frozen. Where and how to inject the adaptors in the base model is still an open question. Lets address the 'where' part of the question first. In the current scenario with a 5-layer MLP, the exact layer for adaptor insertion is not particularly critical. However, in larger models, each layer serves a distinct function and contributes differently to learning. For instance, linear layers convey crucial information, unlike layer normalization in a transformer network. To prevent catastrophic forgetting of the original model, adapter modules must be strategically inserted between these impactful layers. Section 7.1 of the LoRA paper (https://arxiv.org/pdf/2106.09685.pdf) conducts experiments to determine which layers can be effectively used for fine-tuning and which ones should remain undisturbed.\n"
      ],
      "metadata": {
        "id": "h6RzMl8yJzMN"
      },
      "id": "h6RzMl8yJzMN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "922db29b",
        "outputId": "6dcf4709-90f2-422b-9618-5dc8adec0740",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('', __main__.MLP),\n",
              " ('seq', torch.nn.modules.container.Sequential),\n",
              " ('seq.0', torch.nn.modules.linear.Linear),\n",
              " ('seq.1', torch.nn.modules.activation.ReLU),\n",
              " ('seq.2', torch.nn.modules.linear.Linear),\n",
              " ('seq.3', torch.nn.modules.activation.ReLU),\n",
              " ('seq.4', torch.nn.modules.linear.Linear),\n",
              " ('seq.5', torch.nn.modules.activation.LogSoftmax)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Let's identify the names of the modules, ensuring that we fine-tune the appropriate ones with adaptors.\n",
        "[(n, type(m)) for n, m in base_model.named_modules()]"
      ],
      "id": "922db29b"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Where to inject the adapters?\n",
        "\n",
        "In the current scenario with a 5-layer MLP, the exact layer for adapter insertion is not particularly critical. However, in larger models, each layer serves a distinct function and contributes differently to learning. For instance, linear layers convey crucial information, unlike layer normalization in a transformer network. To prevent catastrophic forgetting of the original model, adapter modules must be strategically inserted between these impactful layers. Section 7.1 of the LoRA paper¹ conducts experiments to determine which layers can be effectively used for fine-tuning and which ones should remain undisturbed.\n",
        "\n",
        "Note: Not all layers types can be fine-tuned with LoRA. At the moment, `Linear`, `Embeddings`, `Conv2D` and `Conv1D` are supported."
      ],
      "metadata": {
        "id": "rKB2CQvKmrIt"
      },
      "id": "rKB2CQvKmrIt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to inject the adapters?\n",
        "Now we are going to address the 'how' part of the question. Lets say we choose to place adapters for linear layers seq.0 and seq.2 of the base model. Lets call them Adoptees. Adapters can be placed either in sequence or in parallel to the adoptee layers. Since adapters are small in size compared to adoptee layers, running it in sequence will be inefficient to work with GPU on two counts - GPU memory wont be fully utilized and GPUs are designed for parallel execution so layers in-sequence will cause time inefficiency.\n",
        "\n",
        "Authors of LoRA proposed placing adapters in parallel to the adoptee layers. This design keeps the adoptee's weight matrix and the adapter's matrix separate throughout the fine-tuning process. Both adoptee and adapter must have the same input and output layer dimension so that parallel connection can be accommodated.\n",
        "\n",
        "LoRA proposed decomposing the adapter matrix into two low rank matrices (lora_A and lora_B) which will have very small rank. The adapter with lora_A and lora_B is designed such that the output of their product and output of the adaptee layer are compatible. Only lora_A and lora_B are learned for the specific downstream task.\n",
        "\n",
        "Let's define the LoRA configuration. We set the LoRA rank to 3 and select the layers seq.0 and seq.2 to be used for LoRA fine-tuning. lora_A and lora_B layers are created across both seq.0 and seq.2layers. Number of parameters in lora_A (20 x 3) + number of parameters in lora_B (3 x 2000) == 6060 is much fewer than the number of parameters in seq.0 (20 x 2000) == 40,000. However, the output dimension of lora_A x lora_Band seq.0 are both equal to 2000, irrespective of the value of r! Both the outputs can now be added and passed to the next module of the network."
      ],
      "metadata": {
        "id": "u3OcwccnslZK"
      },
      "id": "u3OcwccnslZK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R31eeOnjKCvq"
      },
      "outputs": [],
      "source": [
        "config = peft.LoraConfig(\n",
        "    r=3,\n",
        "    target_modules=[\"seq.0\", \"seq.2\"],\n",
        ")"
      ],
      "id": "R31eeOnjKCvq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "602b6658",
        "outputId": "af26fa2b-a8f6-4a79-c4bc-64431e37c236"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 12,660 || all params: 455,262 || trainable%: 2.780816321151337\n"
          ]
        }
      ],
      "source": [
        "base_model_pretrained = copy.deepcopy(base_model)  # Let's keep a copy of the pretrained model for later use\n",
        "peft_model = peft.get_peft_model(base_model, config)\n",
        "optimizer = torch.optim.Adam(peft_model.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "peft_model.print_trainable_parameters()"
      ],
      "id": "602b6658"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2103737d"
      },
      "source": [
        "We see that only ~2.8% of parameters are actually trainable, which is what we like to see. Now let's see how the architecture of the model with lora weights look like:"
      ],
      "id": "2103737d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaTdEOIwBbLp",
        "outputId": "a1a15879-577b-46eb-ef09-35e804de1a5c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MLP(\n",
              "      (seq): Sequential(\n",
              "        (0): lora.Linear(\n",
              "          (base_layer): Linear(in_features=20, out_features=2000, bias=True)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Identity()\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=20, out_features=3, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=3, out_features=2000, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "        )\n",
              "        (1): ReLU()\n",
              "        (2): lora.Linear(\n",
              "          (base_layer): Linear(in_features=2000, out_features=200, bias=True)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Identity()\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=2000, out_features=3, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=3, out_features=200, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "        )\n",
              "        (3): ReLU()\n",
              "        (4): Linear(in_features=200, out_features=2, bias=True)\n",
              "        (5): LogSoftmax(dim=-1)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "peft_model"
      ],
      "id": "LaTdEOIwBbLp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "def forward(x):\n",
        "\n",
        "\tseq.0_out = seq.0(x)\n",
        "\tlora_A_out = seq.0.lora_A(x)\n",
        "\tlora_B_out = seq.0.lora_B(lora_A_out)\n",
        "\tlora_B_out = lora_B_out * alpha\n",
        "\tseq.0_lora_out = seq.0_out + lora_B_out\n",
        "\tseq.0_lora_out = ReLU(seq.0_lora_out)\n",
        "\tseq.2(seq.2.lora_out)\n",
        "  \n",
        "  In the pseudo-code above, alpha is a scaling factor that adjusts the magnitude of the combined result (original model output plus low-rank adaptation). This balances the pretrained model’s knowledge and the new task-specific adaptation — by default, alpha is usually set to 1."
      ],
      "metadata": {
        "id": "hI0KkJot8B1Y"
      },
      "id": "hI0KkJot8B1Y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to initialize lora_A and lora_B?\n",
        "If both lora_A and lora_B were initialized to 0, the gradient of the loss with respect to each weight will be the same for all weights and all these neurons will likely undergo the same updates during training. The phenomenon of each neuron learning different aspects of the data is called symmetry breaking. Here, they all will learn the same thing. This is akin to having a single parameter, significantly limiting the model's ability to learn complex patterns. Zero initialization may never cause the symmetry to break.\n",
        "\n",
        "Having both of them randomly initialized may destabilise the training. While this can help break symmetry (as discussed earlier), it can also lead to initial instability. At the beginning of fine-tuning, the network might produce outputs that are significantly off-target. The optimizer has to correct these wrong initializations. There are techniques to mitigate these instabilities and limit the effect of wrong paramters like lower learning rates, smaller initial values, introducing warm up periods during training for smooth transition etc,.\n",
        "\n",
        "LoRA gets best of the both worlds and initializes lora_A with random Gaussian initialization and lora_B is set to 0. This results in the product being 0. There is no inductive bias because in the first few epochs only the base model is in play, adaptors are not contributing to the training - no instabilities during initial training stages.\n",
        "\n",
        "Let's verify this:"
      ],
      "metadata": {
        "id": "bAK6YfI94Uau"
      },
      "id": "bAK6YfI94Uau"
    },
    {
      "cell_type": "code",
      "source": [
        "lora_B = peft_model.state_dict()['base_model.model.seq.0.lora_B.default.weight']\n",
        "lora_A = peft_model.state_dict()['base_model.model.seq.0.lora_A.default.weight']\n",
        "print(lora_B.size())\n",
        "print(lora_A.size())\n",
        "print(torch.all(lora_B @ lora_A == 0))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtZqQCTCokoD",
        "outputId": "fc963003-a6a7-4d44-d9ec-cff86d99f02e"
      },
      "id": "jtZqQCTCokoD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2000, 3])\n",
            "torch.Size([3, 20])\n",
            "tensor(True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional Remark:\n",
        "\n",
        "In Figure 1 of the LoRA paper, the parameters initially named A and B are later referred to as B and A, respectively, starting from Section 4 (Method and Implementation). Despite this switch in nomenclature, it's important to note that both the PEFT implementation and the paper discuss the same parameters. This naming discrepancy is also highlighted in an issue I raised, which can be viewed here: https://github.com/huggingface/peft/issues/983."
      ],
      "metadata": {
        "id": "MFdcABEWWntt"
      },
      "id": "MFdcABEWWntt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "When fine-tuning is performed on the same task and data as used in pretraining, it is observed that the loss remains relatively consistent with the last epoch of pretraining, indicating that the training process remains stable. This stability aligns with the effects of the lora_A and lora_B initialization as proposed in the LoRA paper."
      ],
      "metadata": {
        "id": "XKmdpsd5RMmW"
      },
      "id": "XKmdpsd5RMmW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9200cbc6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "133873a2-6bba-4ce8-b08a-d563f56f3b96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0   train_loss_total=0.1001  eval_loss_total=0.3855\n",
            "epoch=1   train_loss_total=0.0944  eval_loss_total=0.3875\n",
            "epoch=2   train_loss_total=0.0926  eval_loss_total=0.3918\n",
            "epoch=3   train_loss_total=0.0909  eval_loss_total=0.3921\n",
            "epoch=4   train_loss_total=0.0889  eval_loss_total=0.3913\n",
            "CPU times: user 526 ms, sys: 13.1 ms, total: 539 ms\n",
            "Wall time: 641 ms\n"
          ]
        }
      ],
      "source": [
        "%time train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=5)"
      ],
      "id": "9200cbc6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Finetune on the downstream task with peft\n",
        "Let's define a downstream task that is relevant but not identical to the pretraining task. Observing the decreasing trend in the loss indicates effective learning and adaptation to this new task."
      ],
      "metadata": {
        "id": "NvMpyZF-V45r"
      },
      "id": "NvMpyZF-V45r"
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.rand((500, 20)) # Returns a tensor filled with random numbers from a uniform distribution on the interval [0,1).\n",
        "y = (X.sum(1) > 10).long() # y is the label with shape (1000, 1) which results in 1 if the sum of elements in each row is > 10 and 0 otherwise. y is then cast to (torch.int64).\n",
        "n_train = 300\n",
        "batch_size = 64\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[:n_train], y[:n_train]),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        ")\n",
        "eval_dataloader = torch.utils.data.DataLoader(\n",
        "    torch.utils.data.TensorDataset(X[n_train:], y[n_train:]),\n",
        "    batch_size=batch_size,\n",
        ")\n",
        "%time train(peft_model, optimizer, criterion, train_dataloader, eval_dataloader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bPujSdFXH9X",
        "outputId": "d03db632-d695-485f-8a54-2462030b91a3"
      },
      "id": "5bPujSdFXH9X",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch=0   train_loss_total=2.9943  eval_loss_total=2.4758\n",
            "epoch=1   train_loss_total=2.3308  eval_loss_total=1.9671\n",
            "epoch=2   train_loss_total=1.7741  eval_loss_total=1.2765\n",
            "epoch=3   train_loss_total=1.1337  eval_loss_total=0.8434\n",
            "epoch=4   train_loss_total=0.7536  eval_loss_total=0.6921\n",
            "epoch=5   train_loss_total=0.6938  eval_loss_total=0.6921\n",
            "epoch=6   train_loss_total=0.6928  eval_loss_total=0.6921\n",
            "epoch=7   train_loss_total=0.6931  eval_loss_total=0.6921\n",
            "epoch=8   train_loss_total=0.6937  eval_loss_total=0.6921\n",
            "epoch=9   train_loss_total=0.6935  eval_loss_total=0.6921\n",
            "CPU times: user 560 ms, sys: 703 µs, total: 561 ms\n",
            "Wall time: 613 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa55d1d4"
      },
      "source": [
        "To verify the correct application of LoRA, we need to identify which parameters were updated and which remained unchanged. Lets print all the named parameters in pretrained base model and the peft model and compare their weights. Only the extra lora adaptor layers in the peft model should have gotten updated."
      ],
      "id": "fa55d1d4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5C2rrwiuAzYG",
        "outputId": "12a44893-f572-4703-d8a9-5a4589105863"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "** Pretrained based model's parameters **\n",
            "seq.0.weight\n",
            "seq.0.bias\n",
            "seq.2.weight\n",
            "seq.2.bias\n",
            "seq.4.weight\n",
            "seq.4.bias\n",
            "\n",
            "** Peft model's parameters **\n",
            "base_model.model.seq.0.base_layer.weight\n",
            "base_model.model.seq.0.base_layer.bias\n",
            "base_model.model.seq.0.lora_A.default.weight\n",
            "base_model.model.seq.0.lora_B.default.weight\n",
            "base_model.model.seq.2.base_layer.weight\n",
            "base_model.model.seq.2.base_layer.bias\n",
            "base_model.model.seq.2.lora_A.default.weight\n",
            "base_model.model.seq.2.lora_B.default.weight\n",
            "base_model.model.seq.4.weight\n",
            "base_model.model.seq.4.bias\n"
          ]
        }
      ],
      "source": [
        "print(\"** Pretrained based model's parameters **\")\n",
        "for name, param in base_model_pretrained.named_parameters():\n",
        "  print(name)\n",
        "print()\n",
        "print(\"** Peft model's parameters **\")\n",
        "for name, param in peft_model.named_parameters():\n",
        "  print(name)"
      ],
      "id": "5C2rrwiuAzYG"
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.equal(base_model_pretrained.state_dict()['seq.0.weight'], peft_model.state_dict()['base_model.model.seq.0.base_layer.weight']))\n",
        "print(torch.equal(base_model_pretrained.state_dict()['seq.0.bias'], peft_model.state_dict()['base_model.model.seq.0.base_layer.bias']))\n",
        "print(torch.equal(base_model_pretrained.state_dict()['seq.2.weight'], peft_model.state_dict()['base_model.model.seq.2.base_layer.weight']))\n",
        "print(torch.equal(base_model_pretrained.state_dict()['seq.2.bias'], peft_model.state_dict()['base_model.model.seq.2.base_layer.bias']))\n",
        "print(torch.equal(base_model_pretrained.state_dict()['seq.4.weight'], peft_model.state_dict()['base_model.model.seq.4.weight']))\n",
        "print(torch.equal(base_model_pretrained.state_dict()['seq.4.bias'], peft_model.state_dict()['base_model.model.seq.4.bias']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LUq_WZ1hZWOl",
        "outputId": "3739f79b-b9aa-4b33-d9bb-9c81b4b4b20b"
      },
      "id": "LUq_WZ1hZWOl",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxbPiorBrQVz",
        "outputId": "1aedb2ca-1c61-4b3e-83ae-44babd03be92"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 442602 || all params: 442602 || trainable%: 100.0\n",
            "trainable params: 12660 || all params: 455262 || trainable%: 2.780816321151337\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(base_model_pretrained)\n",
        "print_trainable_parameters(peft_model)"
      ],
      "id": "NxbPiorBrQVz"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UcnUhCvbUUlL"
      },
      "source": [
        "12,660 extra paramters are added to the base_model to increase the total number of paramters from 442,602 to 455,262. And only these extra parameters are trainale in the peft_model."
      ],
      "id": "UcnUhCvbUUlL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5oascE80Vpdf"
      },
      "source": [
        "### Finetune with full rank adaptors as well\n",
        "In addition to low rank adaptors, you can also fine-tune full rank adaptors using the peft library. Full rank adaptors are essentially replicas of the layers being adapted. They have the flexibility to be saved independently and later merged. This is another useful feature of the 'peft' library and can be enabled with the \"modules_to_save\" option. In some cases this can increase the performance of the fine-tuning task."
      ],
      "id": "5oascE80Vpdf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b342438f"
      },
      "outputs": [],
      "source": [
        "config_1 = peft.LoraConfig(\n",
        "    r=3,\n",
        "    target_modules=[\"seq.0\", \"seq.2\"],\n",
        "    modules_to_save=[\"seq.4\"],\n",
        ")"
      ],
      "id": "b342438f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FAJI_z2yWTOg",
        "outputId": "5e057a46-cae5-4f39-9887-c186e1d1a377"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 13,062 || all params: 455,664 || trainable%: 2.8665859054039817\n"
          ]
        }
      ],
      "source": [
        "copy_1 = copy.deepcopy(base_model_pretrained) # keep the orginal as is and work on a copy\n",
        "peft_model_1 = peft.get_peft_model(copy_1, config_1)\n",
        "optimizer = torch.optim.Adam(peft_model_1.parameters(), lr=lr)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "peft_model_1.print_trainable_parameters()"
      ],
      "id": "FAJI_z2yWTOg"
    },
    {
      "cell_type": "code",
      "source": [
        "peft_model_1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yDkYklqbA8kO",
        "outputId": "8614610e-1fba-47da-ee03-cbe254d4f663"
      },
      "id": "yDkYklqbA8kO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModel(\n",
              "  (base_model): LoraModel(\n",
              "    (model): MLP(\n",
              "      (seq): Sequential(\n",
              "        (0): lora.Linear(\n",
              "          (base_layer): Linear(in_features=20, out_features=2000, bias=True)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Identity()\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=20, out_features=3, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=3, out_features=2000, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "        )\n",
              "        (1): ReLU()\n",
              "        (2): lora.Linear(\n",
              "          (base_layer): Linear(in_features=2000, out_features=200, bias=True)\n",
              "          (lora_dropout): ModuleDict(\n",
              "            (default): Identity()\n",
              "          )\n",
              "          (lora_A): ModuleDict(\n",
              "            (default): Linear(in_features=2000, out_features=3, bias=False)\n",
              "          )\n",
              "          (lora_B): ModuleDict(\n",
              "            (default): Linear(in_features=3, out_features=200, bias=False)\n",
              "          )\n",
              "          (lora_embedding_A): ParameterDict()\n",
              "          (lora_embedding_B): ParameterDict()\n",
              "        )\n",
              "        (3): ReLU()\n",
              "        (4): ModulesToSaveWrapper(\n",
              "          (original_module): Linear(in_features=200, out_features=2, bias=True)\n",
              "          (modules_to_save): ModuleDict(\n",
              "            (default): Linear(in_features=200, out_features=2, bias=True)\n",
              "          )\n",
              "        )\n",
              "        (5): LogSoftmax(dim=-1)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 504
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n03uYzedaqVg"
      },
      "source": [
        "A replica of `seq.4` is incorporated as-is without applying low rank approximation. This results in the addition of 402 extra parameters to the base model, causing the total number of training parameters to increase by 402. The additional 2 parameters beyond 400 are due to bias being set to true. It's noteworthy that in LoRA layers, the bias is set to False by default."
      ],
      "id": "n03uYzedaqVg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8G1P6_BjZUd_"
      },
      "outputs": [],
      "source": [
        "# %time train(peft_model_1, optimizer, criterion, train_dataloader, eval_dataloader, epochs=max_epochs)"
      ],
      "id": "8G1P6_BjZUd_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Merging the adaptors\n",
        "While parameter efficient finetuning techniques increase inference latency due to the expanded network size with additional adaptor modules, the LoRA adaptors are strategically designed to facilitate merging with adaptee matrices when needed, thereby reducing additional inference time. As demonstrated earlier, the number of parameters in (seq.0.lora_A x seq.0.lora_B) aligns with the number of parameters in seq.0, and similarly for (seq.2.lora_A x seq.2.lora_B) and seq.2. Leveraging this alignment, element-wise addition can be employed during the merging process, optimizing the overall efficiency of the model.\n",
        "\n",
        "Fig will help here."
      ],
      "metadata": {
        "id": "ymKm6VPL_hiM"
      },
      "id": "ymKm6VPL_hiM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R1pPEA_urjT1"
      },
      "outputs": [],
      "source": [
        "peft_model_unmerged = copy.deepcopy(peft_model)\n",
        "peft_model_merged_and_unloaded = peft_model.merge_and_unload()"
      ],
      "id": "R1pPEA_urjT1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gQxqipR5sNZF",
        "outputId": "1ce5bd4e-1703-47c8-f26d-b75d5b4fb03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 0 || all params: 442602 || trainable%: 0.0\n"
          ]
        }
      ],
      "source": [
        "print_trainable_parameters(peft_model_merged_and_unloaded)"
      ],
      "id": "gQxqipR5sNZF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhIB--7gVQru"
      },
      "source": [
        "As we can see above, the total number of parameters is back to 442602 and none are trainable. The inference time now, will be same as it was for the base_model."
      ],
      "id": "UhIB--7gVQru"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7dcde21",
        "outputId": "d2f7df44-13fd-48ed-9b5b-b7fd0dea4a07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New parameter model.seq.0.weight                  |           40000 parameters | not updated\n",
            "New parameter model.seq.0.bias                    |            2000 parameters | not updated\n",
            "New parameter model.seq.0.lora_A.default.weight   |             160 parameters | updated\n",
            "New parameter model.seq.0.lora_B.default.weight   |           16000 parameters | updated\n",
            "New parameter model.seq.2.weight                  |         4000000 parameters | not updated\n",
            "New parameter model.seq.2.bias                    |            2000 parameters | not updated\n",
            "New parameter model.seq.2.lora_A.default.weight   |           16000 parameters | updated\n",
            "New parameter model.seq.2.lora_B.default.weight   |           16000 parameters | updated\n",
            "New parameter model.seq.4.weight                  |            4000 parameters | not updated\n",
            "New parameter model.seq.4.bias                    |               2 parameters | not updated\n"
          ]
        }
      ],
      "source": [
        "for name, param in peft_model.base_model.named_parameters():\n",
        "    if \"lora\" not in name:\n",
        "        print(f\"New parameter {name:<35} | {param.numel():>15} parameters | not updated\")\n",
        "        continue\n",
        "\n",
        "    print(f\"New parameter {name:<35} | {param.numel():>15} parameters | updated\")"
      ],
      "id": "c7dcde21"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b46c6198"
      },
      "source": [
        "## Sharing the model through Hugging Face Hub\n",
        "It is necessary to have a valid Hugging Face account and you need to have 'write access token' to push the model to the hub. You may or may not want to add the token as a git credential. Either ways you will be allowed to login."
      ],
      "id": "b46c6198"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "naF_dWSgHePV",
        "outputId": "30d8352b-caff-444c-9e68-57d3fd2cb0b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Token: \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "# Write token: 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\n",
        "!huggingface-cli login"
      ],
      "id": "naF_dWSgHePV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6289e647"
      },
      "source": [
        "### Pushing the model to HF Hub\n",
        "Create a model id and push the peft_model to Hugging Face Hub."
      ],
      "id": "6289e647"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1b91a0af"
      },
      "outputs": [],
      "source": [
        "user = \"s3pi\"  # put your user name here\n",
        "model_name = \"peft-lora-with-MLP-model_\"\n",
        "model_id = f\"{user}/{model_name}\""
      ],
      "id": "1b91a0af"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122,
          "referenced_widgets": [
            "ada497659b4142a28371317022e84736",
            "47a0de779f9e4ea9a322cd8bf5a90b0e",
            "76e6efab208748af9c0c749e37b58791",
            "000e2271a34b4f7598c5bb3af93cfa5c",
            "70a27d1fdd5643d48b77703ed149e10e",
            "26b07af07fec464599f019b62334eb3b",
            "1f44c41b516947efa7e51436523f5857",
            "26c26f1ee0ec4169b2439362bb8725e3",
            "49032feabf4a408aa19b7737538f8055",
            "81572bfca194455ea0065f2130f4ac25",
            "bcdf9188dac84ca1822dd3d5bce0a245"
          ]
        },
        "id": "1430fffd",
        "outputId": "e5748208-52bb-426d-bfbd-954bbb5c4fb6"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "adapter_model.safetensors:   0%|          | 0.00/51.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ada497659b4142a28371317022e84736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/s3pi/peft-lora-with-MLP-model_/commit/e395ff324600c273782f66b6900698ca366248ac', commit_message='Upload model', commit_description='', oid='e395ff324600c273782f66b6900698ca366248ac', pr_url=None, pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 513
        }
      ],
      "source": [
        "peft_model_unmerged.push_to_hub(model_id)"
      ],
      "id": "1430fffd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "632bd799"
      },
      "source": [
        "As evident, the adapter size is merely 51 kB. Alternatively, this figure can be derived from the 12,660 parameters, each of 32-bit size, resulting in approximately 51KB (12,660 * 4) Bytes. In contrast, the base model comprises 442,602 parameters, amounting to 1,770KB, a size considerably larger than that of the adapter. This size escalation becomes particularly significant in the context of a large language model."
      ],
      "id": "632bd799"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ff78c0c"
      },
      "source": [
        "### Loading the model from HF Hub\n",
        "Now, it only takes one step to load the model from HF Hub. To do this, we can use `PeftModel.from_pretrained`, passing our pretrained base model and the model ID:"
      ],
      "id": "4ff78c0c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce0fcced",
        "outputId": "0f080464-af62-469f-b1f0-4d764913f0cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "peft.peft_model.PeftModel"
            ]
          },
          "metadata": {},
          "execution_count": 469
        }
      ],
      "source": [
        "loaded_model = peft.PeftModel.from_pretrained(base_model_pretrained, model_id)\n",
        "type(loaded_model)"
      ],
      "id": "ce0fcced"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2JYfM2LqJw45",
        "outputId": "8e78f5cf-bf78-4ed1-8a67-a68abc6ab8b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 0 || all params: 455,262 || trainable%: 0.0\n"
          ]
        }
      ],
      "source": [
        "loaded_model.print_trainable_parameters()"
      ],
      "id": "2JYfM2LqJw45"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmlPkR3tNRqN",
        "outputId": "1d815358-00b4-4d75-dcf7-ca9531d2c030"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 0 || all params: 442602 || trainable%: 0.0\n"
          ]
        }
      ],
      "source": [
        "loaded_model_merged_and_unloaded = loaded_model.merge_and_unload()\n",
        "print_trainable_parameters(merged_model)"
      ],
      "id": "NmlPkR3tNRqN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cd4b4eac"
      },
      "source": [
        "Let's check that the two models produce the same output:"
      ],
      "id": "cd4b4eac"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2cf6ac4",
        "outputId": "792ea1cc-7253-4a40-c4b5-0846b8ad2c6f",
        "scrolled": true
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 472
        }
      ],
      "source": [
        "y_peft = peft_model_merged_and_unloaded(X.to(device))\n",
        "y_loaded = loaded_model_merged_and_unloaded(X.to(device))\n",
        "torch.allclose(y_peft, y_loaded)"
      ],
      "id": "f2cf6ac4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeeb653f"
      },
      "source": [
        "### Clean up\n",
        "Finally, as a clean up step, you may want to delete the repo."
      ],
      "id": "eeeb653f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b747038f"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import delete_repo"
      ],
      "id": "b747038f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7e5ab237"
      },
      "outputs": [],
      "source": [
        "delete_repo(model_id)"
      ],
      "id": "7e5ab237"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performance\n",
        "\n",
        "In Section 7 of the LoRA paper, it's demonstrated that low-rank adaptation matrices can enhance important features for specific downstream tasks—features that were initially learned but not strongly emphasized in the general pre-training model. They employ a metric called amplification factor to show the same. that a small r (such as 2) can yield a higher amplification factor compared to a larger r (like 64). This finding implies that only a limited number of directions (or features, in this case, 2) in the weight space are crucial for adapting the model to a specific task.This insight is valuable for efficiently adapting large pre-trained models like GPT-3, as it indicates that only a small number of directions need to be modified for specific tasks, the number of parameters that need to be trained. For different downstream tasks, a distinct set of feature directions are likely to be amplified.\n"
      ],
      "metadata": {
        "id": "zuLOREC82jEY"
      },
      "id": "zuLOREC82jEY"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations\n",
        "- LoRA adaptors are only few mega bytes whereas the pretrained model is several gigabytes, during inference we need both - so not much of a saving in terms of memory requirement during inference although significantly less memory is required during fine tuning compared to fine tuning all the layers of the pretrained model. QLoRA is the solution to this problem.\n",
        "- For each sub task, an adaptor is trained. If a batch has data for multiple tasks, cannot load multiple adapters at the same time."
      ],
      "metadata": {
        "id": "cSlnMmPoQLgt"
      },
      "id": "cSlnMmPoQLgt"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Extras"
      ],
      "metadata": {
        "id": "5aqqPXD2WVBC"
      },
      "id": "5aqqPXD2WVBC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Amplification Factor"
      ],
      "metadata": {
        "id": "HbVjkvT_q60E"
      },
      "id": "HbVjkvT_q60E"
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "Delta_W = peft_model.state_dict()['base_model.model.seq.2.lora_B.default.weight'] @ peft_model.state_dict()['base_model.model.seq.2.lora_A.default.weight']\n",
        "W = base_model_pretrained.state_dict()['seq.2.weight'].numpy()\n",
        "\n",
        "U, _, VT = np.linalg.svd(Delta_W)\n",
        "W_projected = U.T @ W @ VT\n",
        "norm_Delta_W = np.linalg.norm(Delta_W, 'fro')\n",
        "norm_W_projected = np.linalg.norm(W_projected, 'fro')\n",
        "amplification_factor = norm_Delta_W / norm_W_projected\n",
        "norm_Delta_W, norm_W_projected, amplification_factor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qfUvPC6blb0",
        "outputId": "0672cf7d-676c-49fe-829c-136336928da3"
      },
      "id": "3qfUvPC6blb0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.79638445, 17.702658, 0.044986717)"
            ]
          },
          "metadata": {},
          "execution_count": 117
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Low Rank Approximation"
      ],
      "metadata": {
        "id": "EbIVi6FNxYwX"
      },
      "id": "EbIVi6FNxYwX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Create a random 10x20 matrix\n",
        "A = np.random.rand(5, 5)\n",
        "\n",
        "# Perform Singular Value Decomposition (SVD)\n",
        "U, Sigma, VT = np.linalg.svd(A, full_matrices=False)\n",
        "\n",
        "# Choose the rank for approximation\n",
        "rank = 3\n",
        "\n",
        "# Construct low-rank matrices B and C\n",
        "B = U[:, :rank]\n",
        "C = np.dot(np.diag(Sigma[:rank]), VT[:rank, :])\n",
        "\n",
        "# Construct an approximation of the original matrix using the selected rank\n",
        "A_approx = np.dot(B, C)\n",
        "\n",
        "# Print the original matrix A, low-rank matrices B and C, and the approximated matrix A_approx\n",
        "print(\"Original Matrix A:\")\n",
        "print(np.round(A, 2))\n",
        "print(\"\\nLow-rank Matrix B:\")\n",
        "print(np.round(B, 2))\n",
        "print(\"\\nLow-rank Matrix C:\")\n",
        "print(np.round(C, 2))\n",
        "print(\"\\nApproximated Matrix A_approx:\")\n",
        "print(np.round(A_approx, 2))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVuvtq_ZAice",
        "outputId": "e3abd788-ade4-4241-cb25-b4e5897158f3"
      },
      "id": "yVuvtq_ZAice",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Matrix A:\n",
            "[[0.97 0.57 0.71 0.15 0.12]\n",
            " [0.5  0.34 0.19 0.76 0.46]\n",
            " [0.08 0.17 0.45 0.79 0.68]\n",
            " [0.08 0.28 0.13 0.07 0.44]\n",
            " [0.77 0.92 0.06 0.1  0.47]]\n",
            "\n",
            "Low-rank Matrix B:\n",
            "[[-0.55  0.46  0.67]\n",
            " [-0.46 -0.34  0.  ]\n",
            " [-0.4  -0.71  0.12]\n",
            " [-0.2  -0.1  -0.34]\n",
            " [-0.53  0.4  -0.65]]\n",
            "\n",
            "Low-rank Matrix C:\n",
            "[[-1.22 -1.08 -0.72 -0.82 -0.89]\n",
            " [ 0.51  0.36 -0.05 -0.72 -0.45]\n",
            " [ 0.13 -0.29  0.45  0.11 -0.29]]\n",
            "\n",
            "Approximated Matrix A_approx:\n",
            "[[ 0.99  0.57  0.67  0.19  0.08]\n",
            " [ 0.39  0.38  0.35  0.62  0.56]\n",
            " [ 0.14  0.14  0.37  0.85  0.64]\n",
            " [ 0.15  0.28 -0.    0.2   0.32]\n",
            " [ 0.77  0.91  0.07  0.09  0.49]]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ada497659b4142a28371317022e84736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_47a0de779f9e4ea9a322cd8bf5a90b0e",
              "IPY_MODEL_76e6efab208748af9c0c749e37b58791",
              "IPY_MODEL_000e2271a34b4f7598c5bb3af93cfa5c"
            ],
            "layout": "IPY_MODEL_70a27d1fdd5643d48b77703ed149e10e"
          }
        },
        "47a0de779f9e4ea9a322cd8bf5a90b0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26b07af07fec464599f019b62334eb3b",
            "placeholder": "​",
            "style": "IPY_MODEL_1f44c41b516947efa7e51436523f5857",
            "value": "adapter_model.safetensors: 100%"
          }
        },
        "76e6efab208748af9c0c749e37b58791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26c26f1ee0ec4169b2439362bb8725e3",
            "max": 51080,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_49032feabf4a408aa19b7737538f8055",
            "value": 51080
          }
        },
        "000e2271a34b4f7598c5bb3af93cfa5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81572bfca194455ea0065f2130f4ac25",
            "placeholder": "​",
            "style": "IPY_MODEL_bcdf9188dac84ca1822dd3d5bce0a245",
            "value": " 51.1k/51.1k [00:00&lt;00:00, 110kB/s]"
          }
        },
        "70a27d1fdd5643d48b77703ed149e10e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26b07af07fec464599f019b62334eb3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f44c41b516947efa7e51436523f5857": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "26c26f1ee0ec4169b2439362bb8725e3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "49032feabf4a408aa19b7737538f8055": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81572bfca194455ea0065f2130f4ac25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bcdf9188dac84ca1822dd3d5bce0a245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}